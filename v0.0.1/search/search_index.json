{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#kube-bench","title":"Kube-bench","text":"<p>kube-bench is a Go application that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark.</p> <p>Tests are configured with YAML files, making this tool easy to update as test specifications evolve.</p> <ol> <li> <p>kube-bench implements the CIS Kubernetes Benchmark as closely as possible. Please raise issues here if kube-bench is not correctly implementing the test as described in the Benchmark. To report issues in the Benchmark itself (for example, tests that you believe are inappropriate), please join the CIS community.</p> </li> <li> <p>There is not a one-to-one mapping between releases of Kubernetes and releases of the CIS benchmark. See CIS Kubernetes Benchmark support to see which releases of Kubernetes are covered by different releases of the benchmark.</p> </li> <li> <p>It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments.</p> </li> </ol> <p>For help and more information go to our github discussions q&amp;a</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#test-config-yaml-representation","title":"Test config YAML representation","text":"<p>The tests (or \"controls\") are maintained in YAML documents. There are different versions of these test YAML files reflecting different versions and platforms of the CIS Kubernetes Benchmark. You will find more information about the test file YAML definitions in our controls documentation.</p>"},{"location":"architecture/#kube-bench-benchmarks","title":"Kube-bench benchmarks","text":"<p>The test files for the various versions of Benchmarks can be found in directories with same name as the Benchmark versions under the <code>cfg</code> directory next to the kube-bench executable,  for example <code>./cfg/cis-1.5</code> will contain all test files for CIS Kubernetes Benchmark v1.5.1 which are: master.yaml, controlplane.yaml, node.yaml, etcd.yaml, policies.yaml and config.yaml </p> <p>Check the contents of the benchmark directory under <code>cfg</code> to see which targets are available for that benchmark. Each file except <code>config.yaml</code> represents a target (also known as a <code>control</code> in other parts of this documentation). </p> <p>The following table shows the valid targets based on the CIS Benchmark version.</p> CIS Benchmark Targets cis-1.5 master, controlplane, node, etcd, policies cis-1.6 master, controlplane, node, etcd, policies cis-1.20 master, controlplane, node, etcd, policies cis-1.23 master, controlplane, node, etcd, policies cis-1.24 master, controlplane, node, etcd, policies cis-1.7 master, controlplane, node, etcd, policies cis-1.8 master, controlplane, node, etcd, policies cis-1.9 master, controlplane, node, etcd, policies gke-1.0 master, controlplane, node, etcd, policies, managedservices gke-1.2.0 controlplane, node, policies, managedservices gke-1.6.0 controlplane, node, policies, managedservices eks-1.0.1 controlplane, node, policies, managedservices eks-1.1.0 controlplane, node, policies, managedservices eks-1.2.0 controlplane, node, policies, managedservices ack-1.0 master, controlplane, node, etcd, policies, managedservices aks-1.0 controlplane, node, policies, managedservices rh-0.7 master,node rh-1.0 master, controlplane, node, etcd, policies cis-1.6-k3s master, controlplane, node, etcd, policies cis-1.24-microk8s master, controlplane, node, etcd, policies <p>The following table shows the valid DISA STIG versions</p> STIG Targets eks-stig-kubernetes-v1r6 master, controlplane, node, policies, managedservices"},{"location":"asff/","title":"Integrating kube-bench with AWS Security Hub","text":"<p>You can configure kube-bench with the <code>--asff</code> to send findings to AWS Security Hub. There are some additional steps required so that kube-bench has information and permissions to send these findings.</p>"},{"location":"asff/#enable-the-aws-security-hub-integration","title":"Enable the AWS Security Hub integration","text":"<ul> <li>You will need AWS Security Hub to be enabled in your account</li> <li>In the Security Hub console, under Integrations, search for kube-bench</li> </ul> <ul> <li>Click on <code>Accept findings</code>. This gives information about the IAM permissions required to send findings to your Security Hub account. kube-bench runs within a pod on your EKS cluster, and will need to be associated with a Role that has these permissions.</li> </ul>"},{"location":"asff/#configure-permissions-in-an-iam-role","title":"Configure permissions in an IAM Role","text":"<ul> <li>Grant these permissions to the IAM Role that the kube-bench pod will be associated with. There are two options:</li> <li>You can run the kube-bench pod under a specific service account associated with an IAM role that has these permissions to write Security Hub findings.</li> <li>Alternatively the pod can be granted permissions specified by the Role that your EKS node group uses.</li> </ul> <p>Here is an example IAM Policy that you can attach to your EKS node group's IAM Role: </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"securityhub:BatchImportFindings\",\n            \"Resource\": [\n                \"arn:aws:securityhub:us-east-1::product/khulnasoft/kube-bench\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"asff/#modify-the-job-configuration","title":"Modify the job configuration","text":"<ul> <li>Modify the kube-bench Configmap in <code>job-eks-asff.yaml</code> to specify the AWS account, AWS region, and the EKS Cluster ARN.</li> <li>In the same file, modify the image specifed in the Job to use the kube-bench image pushed to your ECR</li> <li>[Optional] - If you have created a dedicated IAM role to be used with kube-bench as described above in Configure permissions in an IAM Role, you will need to add the IAM role arn to the kube-bench ServiceAccount in <code>job-eks-asff.yaml</code>.</li> <li>Make sure that <code>job-eks-asff.yaml</code> specifies the container image you just pushed to your ECR registry.</li> </ul> <p>You can now run kube-bench as a pod in your cluster: <code>kubectl apply -f job-eks-asff.yaml</code></p> <p>Findings will be generated for any kube-bench test that generates a <code>[FAIL]</code> or <code>[WARN]</code> output. If all tests pass, no findings will be generated. However, it's recommended that you consult the pod log output to check whether any findings were generated but could not be written to Security Hub.</p> <p> </p>"},{"location":"controls/","title":"Test and config files","text":"<p><code>kube-bench</code> runs checks specified in <code>controls</code> files that are a YAML  representation of the CIS Kubernetes Benchmark checks (or other distribution-specific hardening guides). </p>"},{"location":"controls/#controls","title":"Controls","text":"<p><code>controls</code> is a YAML document that contains checks that must be run against a  specific Kubernetes node type, master or node and version.</p> <p><code>controls</code> is the fundamental input to <code>kube-bench</code>. The following is an example  of a basic <code>controls</code>:</p> <pre><code>---\ncontrols:\nid: 1\ntext: \"Master Node Security Configuration\"\ntype: \"master\"\ngroups:\n- id: 1.1\n  text: API Server\n  checks:\n    - id: 1.1.1\n      text: \"Ensure that the --allow-privileged argument is set (Scored)\"\n      audit: \"ps -ef | grep kube-apiserver | grep -v grep\"\n      tests:\n      bin_op: or\n      test_items:\n      - flag: \"--allow-privileged\"\n        set: true\n      - flag: \"--some-other-flag\"\n        set: false\n      remediation: \"Edit the /etc/kubernetes/config file on the master node and\n        set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\"\n      scored: true\n- id: 1.2\n  text: Scheduler\n  checks:\n    - id: 1.2.1\n      text: \"Ensure that the --profiling argument is set to false (Scored)\"\n      audit: \"ps -ef | grep kube-scheduler | grep -v grep\"\n      tests:\n        bin_op: and\n        test_items:\n          - flag: \"--profiling\"\n            set: true\n          - flag: \"--some-other-flag\"\n            set: false\n      remediation: \"Edit the /etc/kubernetes/config file on the master node and\n        set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\"\n      scored: true\n</code></pre> <p><code>controls</code> is composed of a hierarchy of groups, sub-groups and checks. Each of the <code>controls</code> components have an id and a text description which are displayed  in the <code>kube-bench</code> output.</p> <p><code>type</code> specifies what kubernetes node type a <code>controls</code> is for. Possible values for <code>type</code> are <code>master</code> and <code>node</code>.</p>"},{"location":"controls/#groups","title":"Groups","text":"<p><code>groups</code> is a list of subgroups that test the various Kubernetes components that run on the node type specified in the <code>controls</code>. </p> <p>For example, one subgroup checks parameters passed to the API server binary, while  another subgroup checks parameters passed to the controller-manager binary.</p> <pre><code>groups:\n- id: 1.1\n  text: API Server\n  # ...\n- id: 1.2\n  text: Scheduler\n  # ...\n</code></pre> <p>These subgroups have <code>id</code>, <code>text</code> fields which serve the same purposes described in the previous paragraphs. The most important part of the subgroup is the <code>checks</code> field which is the collection of actual <code>check</code>s that form the subgroup.</p> <p>This is an example of a subgroup and checks in the subgroup.</p> <pre><code>id: 1.1\ntext: API Server\nchecks:\n  - id: 1.1.1\n    text: \"Ensure that the --allow-privileged argument is set (Scored)\"\n    audit: \"ps -ef | grep kube-apiserver | grep -v grep\"\n    tests:\n    # ...\n  - id: 1.1.2\n    text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\"\n    audit: \"ps -ef | grep kube-apiserver | grep -v grep\"\n    tests:\n    # ...\n</code></pre> <p><code>kube-bench</code> supports running a subgroup by specifying the subgroup <code>id</code> on the command line, with the flag <code>--group</code> or <code>-g</code>.</p>"},{"location":"controls/#check","title":"Check","text":"<p>The CIS Kubernetes Benchmark recommends configurations to harden Kubernetes components. These recommendations are usually configuration options and can be  specified by flags to Kubernetes binaries, or in configuration files.</p> <p>The Benchmark also provides commands to audit a Kubernetes installation, identify places where the cluster security can be improved, and steps to remediate these identified problems.</p> <p>In <code>kube-bench</code>, <code>check</code> objects embody these recommendations.  This an example <code>check</code> object:</p> <pre><code>id: 1.1.1\ntext: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\"\naudit: \"ps -ef | grep kube-apiserver | grep -v grep\"\ntests:\n  test_items:\n  - flag: \"--anonymous-auth\"\n    compare:\n      op: eq\n      value: false\n    set: true\nremediation: |\n  Edit the API server pod specification file kube-apiserver\n  on the master node and set the below parameter.\n  --anonymous-auth=false\nscored: false\n</code></pre> <p>A <code>check</code> object has an <code>id</code>, a <code>text</code>, an <code>audit</code>, a <code>tests</code>, <code>remediation</code> and <code>scored</code> fields.</p> <p><code>kube-bench</code> supports running individual checks by specifying the check's <code>id</code> as a comma-delimited list on the command line with the <code>--check</code> flag.</p> <p>The <code>audit</code> field specifies the command to run for a check. The output of this command is then evaluated for conformance with the CIS Kubernetes Benchmark recommendation.</p> <p>The audit is evaluated against criteria specified by the <code>tests</code> object. <code>tests</code> contain <code>bin_op</code> and <code>test_items</code>.</p> <p><code>test_items</code> specify the criteria(s) the <code>audit</code> command's output should meet to pass a check. This criteria is made up of keywords extracted from the output of the <code>audit</code> command and operations that compare these keywords against values expected by the CIS Kubernetes Benchmark. </p> <p>There are three ways to run and extract keywords from the output of the command used,  | Command | Output var | |---|---| | <code>audit</code> | <code>flag</code> | | <code>audit_config</code> | <code>path</code> |  | <code>audit_env</code> | <code>env</code> |</p> <p><code>flag</code> is used when the keyword is a command-line flag. The associated <code>audit</code> command could  be any binaries available on the system like <code>ps</code> command and a <code>grep</code> for the binary whose flag we are checking:</p> <pre><code>ps -ef | grep somebinary | grep -v grep\n</code></pre> <p>Here is an example usage of the <code>flag</code> option:</p> <pre><code># ...\naudit: \"ps -ef | grep kube-apiserver | grep -v grep\"\ntests:\n  test_items:\n  - flag: \"--anonymous-auth\"\n  # ...\n</code></pre> <p><code>path</code> is used when the keyword is an option set in a JSON or YAML config file. The associated <code>audit_command</code> command is usually <code>cat /path/to/config-yaml-or-json</code>. For example:</p> <pre><code># ...\ntext: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\"\naudit: \"cat /path/to/some/config\"\ntests:\n  test_items:\n  - path: \"{.someoption.value}\"\n    # ...\n</code></pre> <p><code>env</code> is used to check if the value is present within a specified environment variable. The presence of <code>env</code> is treated as an OR operation, if both <code>flag</code> and <code>env</code> are supplied it will use either to attempt pass the check. The command used for checking the environment variables of a process is generated by default.</p> <p>If the command being generated is causing errors, you can override the command used by setting <code>audit_env</code> on the check. Similarly, if you don't want the environment checking command to be generated or run at all, specify <code>disableEnvTesting</code> as true on the check.</p> <p>The example below will check if the flag <code>--auto-tls</code> is equal to false OR <code>ETCD_AUTO_TLS</code> is equal to false</p> <p><pre><code>  test_items:\n  - flag: \"--auto-tls\"\n    env: \"ETCD_AUTO_TLS\"\n    compare:\n      op: eq\n      value: false\n</code></pre> Note: flag, path and env will act as OR if more then one present. </p> <p><code>test_item</code> compares the output of the audit command and keywords using the <code>set</code> and <code>compare</code> fields.</p> <pre><code>  test_items:\n  - flag: \"--anonymous-auth\"\n    compare:\n      op: eq\n      value: false\n    set: true\n</code></pre> <p><code>set</code> checks if a keyword is present in the output of the audit command or a config file. The possible values for <code>set</code> are true and false.</p> <p>If <code>set</code> is true, the check passes only if the keyword is present in the output of the audit command, or config file. If <code>set</code> is false, the check passes only if the keyword is not present in the output of the audit command, or config file. <code>set</code> is true by default.</p> <p><code>compare</code> has two fields <code>op</code> and <code>value</code> to compare keywords with expected value. <code>op</code> specifies which operation is used for the comparison, and <code>value</code> specifies the value to compare against.</p> <p>To use <code>compare</code>, <code>set</code> must true. The comparison will be ignored if <code>set</code> is false</p> <p>The <code>op</code> (operations) currently supported in <code>kube-bench</code> are: - <code>eq</code>: tests if the keyword is equal to the compared value. - <code>noteq</code>: tests if the keyword is unequal to the compared value. - <code>gt</code>: tests if the keyword is greater than the compared value. - <code>gte</code>: tests if the keyword is greater than or equal to the compared value. - <code>lt</code>: tests if the keyword is less than the compared value. - <code>lte</code>: tests if the keyword is less than or equal to the compared value. - <code>has</code>: tests if the keyword contains the compared value. - <code>nothave</code>: tests if the keyword does not contain the compared value. - <code>regex</code>: tests if the flag value matches the compared value regular expression.    When defining regular expressions in YAML it is generally easier to wrap them in    single quotes, for example <code>'^[abc]$'</code>, to avoid issues with string escaping. - <code>bitmask</code> : tests if keyward is bitmasked with the compared value, common usege is for     comparing file permissions in linux.</p>"},{"location":"controls/#omitting-checks","title":"Omitting checks","text":"<p>If you decide that a recommendation is not appropriate for your environment, you can choose to omit it by editing the test YAML file to give it the check type <code>skip</code> as in this example:</p> <pre><code>  checks:\n  - id: 2.1.1\n    text: \"Ensure that the --allow-privileged argument is set to false (Scored)\"\n    type: \"skip\"\n    scored: true\n</code></pre> <p>No tests will be run for this check and the output will be marked [INFO].</p>"},{"location":"controls/#configuration-and-variables","title":"Configuration and Variables","text":"<p>Kubernetes component configuration and binary file locations and names  vary based on cluster deployment methods and Kubernetes distribution used. For this reason, the locations of these binaries and config files are configurable by editing the <code>cfg/config.yaml</code> file and these binaries and files can be referenced in a <code>controls</code> file via variables.</p> <p>The <code>cfg/config.yaml</code> file is a global configuration file. Configuration files can be created for specific Kubernetes versions (distributions). Values in the version-specific config overwrite similar values in <code>cfg/config.yaml</code>.</p> <p>For example, the kube-apiserver in Red Hat OCP distribution is run as  <code>hypershift openshift-kube-apiserver</code> instead of the default <code>kube-apiserver</code>. This difference can be specified by editing the <code>master.apiserver.defaultbin</code> entry <code>cfg/rh-0.7/config.yaml</code>.</p> <p>Below is the structure of <code>cfg/config.yaml</code>:</p> <pre><code>nodetype\n  |-- components\n    |-- component1\n  |-- component1\n    |-- bins\n    |-- defaultbin (optional)\n    |-- confs\n    |-- defaultconf (optional)\n    |-- svcs\n    |-- defaultsvc (optional)\n    |-- kubeconfig\n    |-- defaultkubeconfig (optional)\n</code></pre> <p>Every node type has a subsection that specifies the main configuration items.</p> <ul> <li><code>components</code>: A list of components for the node type. For example master    will have an entry for apiserver, scheduler and controllermanager.</li> </ul> <p>Each component has the following entries:</p> <ul> <li><code>bins</code>: A list of candidate binaries for a component. <code>kube-bench</code> checks this    list and selects the first binary that is running on the node.</li> </ul> <p>If none of the binaries in <code>bins</code> list is running, <code>kube-bench</code> checks if the    binary specified by <code>defaultbin</code> is running and terminates if none of the     binaries in both <code>bins</code> and <code>defaultbin</code> is running.</p> <p>The selected binary for a component can be referenced in <code>controls</code> using a     variable in the form <code>$&lt;component&gt;bin</code>. In the example below, we reference     the selected API server binary with the variable <code>$apiserverbin</code> in an <code>audit</code>    command.</p> <pre><code>id: 1.1.1\n text: \"Ensure that the --anonymous-auth argument is set to false (Scored)\"\n audit: \"ps -ef | grep $apiserverbin | grep -v grep\"\n # ...\n</code></pre> <ul> <li><code>confs</code>: A list of candidate configuration files for a component. <code>kube-bench</code>   checks this list and selects the first config file that is found on the node.   If none of the config files exists, <code>kube-bench</code> defaults conf to the value   of <code>defaultconf</code>.</li> </ul> <p>The selected config for a component can be referenced in <code>controls</code> using a   variable in the form <code>$&lt;component&gt;conf</code>. In the example below, we reference the    selected API server config file with the variable <code>$apiserverconf</code> in an <code>audit</code>   command.</p> <pre><code>id: 1.4.1\n  text: \"Ensure that the API server pod specification file permissions are\n  set to 644 or more restrictive (Scored)\"\n  audit: \"/bin/sh -c 'if test -e $apiserverconf; then stat -c %a $apiserverconf; fi'\"\n</code></pre> <ul> <li><code>svcs</code>:  A list of candidate unitfiles for a component. <code>kube-bench</code> checks this    list and selects the first unitfile that is found on the node. If none of the   unitfiles exists, <code>kube-bench</code> defaults unitfile to the value of <code>defaultsvc</code>.</li> </ul> <p>The selected unitfile for a component can be referenced in <code>controls</code> via a   variable in the form <code>$&lt;component&gt;svc</code>. In the example below, the selected    kubelet unitfile is referenced with <code>$kubeletsvc</code> in the <code>remediation</code> of the    <code>check</code>.</p> <pre><code>id: 2.1.1\n  # ...\n  remediation: |\n    Edit the kubelet service file $kubeletsvc\n    on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n    --allow-privileged=false\n    Based on your system, restart the kubelet service. For example:\n    systemctl daemon-reload\n    systemctl restart kubelet.service\n  # ...\n</code></pre> <ul> <li> <p><code>kubeconfig</code>: A list of candidate kubeconfig files for a component. <code>kube-bench</code>     checks this list and selects the first file that is found on the node. If none     of the files exists, <code>kube-bench</code> defaults kubeconfig to the value of      <code>defaultkubeconfig</code>.</p> <p>The selected kubeconfig for a component can be referenced in <code>controls</code> with a variable in the form <code>$&lt;component&gt;kubeconfig</code>. In the example below, the selected kubelet kubeconfig is referenced with <code>$kubeletkubeconfig</code> in the <code>audit</code> command.</p> <pre><code>id: 2.2.1\n  text: \"Ensure that the kubelet.conf file permissions are set to 644 or\n  more restrictive (Scored)\"\n  audit: \"/bin/sh -c 'if test -e $kubeletkubeconfig; then stat -c %a $kubeletkubeconfig; fi'\"\n  # ...\n</code></pre> </li> </ul>"},{"location":"flags-and-commands/","title":"Flags","text":""},{"location":"flags-and-commands/#commands","title":"Commands","text":"Command Description help Prints help about any command run List of components to run version Print kube-bench version"},{"location":"flags-and-commands/#flags","title":"Flags","text":"Flag Description --alsologtostderr log to standard error as well as files --asff Send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See [this page][kube-bench-aws-security-hub] for more information on how to enable the kube-bench integration with AWS Security Hub. --benchmark Manually specify CIS benchmark version -c, --check A comma-delimited list of checks to run as specified in Benchmark document. --config config file (default is ./cfg/config.yaml) --exit-code Specify the exit code for when checks fail --group Run all the checks under this comma-delimited list of groups. --include-test-output Prints the actual result when test fails. --json Prints the results as JSON --junit Prints the results as JUnit --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --logtostderr log to standard error instead of files --noremediations Disable printing of remediations section to stdout. --noresults Disable printing of results section to stdout. --nototals Disable calculating and printing of totals for failed, passed, ... checks across all sections --outputfile Writes the results to output file when run with --json or --junit --pgsql Save the results to PostgreSQL --scored Run the scored CIS checks (default true) --skip string List of comma separated values of checks to be skipped --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs (default 0) --unscored Run the unscored CIS checks (default true) --version string Manually specify Kubernetes version, automatically detected if unset --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging"},{"location":"flags-and-commands/#examples","title":"Examples","text":""},{"location":"flags-and-commands/#report-kube-bench-findings-to-aws-security-hub","title":"Report kube-bench findings to AWS Security Hub","text":"<p>You can configure kube-bench with the <code>--asff</code> option to send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See this page for more information on how to enable the kube-bench integration with AWS Security Hub.</p>"},{"location":"flags-and-commands/#specifying-the-benchmark-or-kubernetes-version","title":"Specifying the benchmark or Kubernetes version","text":"<p><code>kube-bench</code> uses the Kubernetes API, or access to the <code>kubectl</code> or <code>kubelet</code> executables to try to determine the Kubernetes version, and hence which benchmark to run. If you wish to override this, or if none of these methods are available, you can specify either the Kubernetes version or CIS Benchmark as a command line parameter.  </p> <p>You can specify a particular version of Kubernetes by setting the <code>--version</code> flag or with the <code>KUBE_BENCH_VERSION</code> environment variable. The value of <code>--version</code> takes precedence over the value of <code>KUBE_BENCH_VERSION</code>.</p> <p>For example, run kube-bench using the tests for Kubernetes version 1.13:</p> <pre><code>kube-bench --version 1.13\n</code></pre> <p>You can specify <code>--benchmark</code> to run a specific CIS Benchmark version:</p> <pre><code>kube-bench --benchmark cis-1.5\n</code></pre> <p>Note:  It is an error to specify both <code>--version</code> and <code>--benchmark</code> flags together</p>"},{"location":"flags-and-commands/#specifying-benchmark-sections","title":"Specifying Benchmark sections","text":"<p>If you want to run specific CIS Benchmark sections (i.e master, node, etcd, etc...) you can use the <code>run --targets</code> subcommand.</p> <pre><code>kube-bench run --targets master,node\n</code></pre> <p>or</p> <pre><code>kube-bench run --targets master,node,etcd,policies\n</code></pre> <p>If no targets are specified, <code>kube-bench</code> will determine the appropriate targets based on the CIS Benchmark version and the components detected on the node. The detection is done by verifying which components are running, as defined in the config files (see Configuration.</p>"},{"location":"flags-and-commands/#run-specific-check-or-group","title":"Run specific check or group","text":"<p><code>kube-bench</code> supports running individual checks by specifying the check's <code>id</code> as a comma-delimited list on the command line with the <code>--check</code> | <code>-c</code> flag. <code>kube-bench --check=\"1.1.1,1.1.2,1.2.1,1.3.3\"</code></p> <p><code>kube-bench</code> supports running all checks under group by specifying the group's <code>id</code> as a comma-delimited list on the command line with the <code>--group</code> | <code>-g</code> flag. <code>kube-bench --check=\"1.1,2.2\"</code> Will run all checks 1.1.X and 2.2.X. </p>"},{"location":"flags-and-commands/#skip-specific-check-or-group","title":"Skip specific check or group","text":"<p><code>kube-bench</code> supports skipping checks or groups by specifying the <code>id</code> as a comma-delimited list on the command line with the <code>--skip</code> flag. <code>kube-bench --skip=\"1.1,1.2.1,1.3.3\"</code> Will skip 1.1.X group and individual checks 1.2.1, 1.3.3. Skipped checks returns [INFO] output. </p>"},{"location":"flags-and-commands/#exit-code","title":"Exit code","text":"<p><code>kube-bench</code> supports using uniqe exit code when failing a check or more.  <code>kube-bench --exit-code 42</code>  Will return 42 if one check or more failed, and 0 incase none failed.  Note: [WARN] is not [FAIL].</p>"},{"location":"flags-and-commands/#output-manipulation-flags","title":"Output manipulation flags","text":"<p>There are four output states: - [PASS] indicates that the test was run successfully, and passed. - [FAIL] indicates that the test was run successfully, and failed. The remediation output describes how to correct the configuration, or includes an error message describing why the test could not be run. - [WARN] means this test needs further attention, for example it is a test that needs to be run manually. Check the remediation output for further information. - [INFO] is informational output that needs no further action.</p> <p>Note: - Some tests with <code>Automated</code> in their description must still be run manually - If the user has to run a test manually, this always generates WARN - If the test is Scored, and kube-bench was unable to run the test, this generates FAIL (because the test has not been passed, and as a Scored test, if it doesn't pass then it must be considered a failure). - If the test is Not Scored, and kube-bench was unable to run the test, this generates WARN. - If the test is Scored, type is empty, and there are no <code>test_items</code> present, it generates a WARN. This is to highlight tests that appear to be incompletely defined.</p> <p><code>kube-bench</code> supports multiple output manipulation flags.  <code>kube-bench --include-test-output</code> will print failing checks output in the results section <pre><code>[INFO] 1 Master Node Security Configuration\n[INFO] 1.1 Master Node Configuration Files\n[FAIL] 1.1.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated)\n         **permissions=777**\n</code></pre></p> <p>Note: <code>--noresults</code> <code>--noremediations</code> and <code>--include-test-output</code> will not effect the json output but only stdout.  Only <code>--nototals</code> will effect the json output and thats because it will not call the function to calculate totals. </p>"},{"location":"flags-and-commands/#troubleshooting","title":"Troubleshooting","text":"<p>Running <code>kube-bench</code> with the <code>-v 3</code> parameter will generate debug logs that can be very helpful for debugging problems.</p> <p>If you are using one of the example <code>job*.yaml</code> files, you will need to edit the <code>command</code> field, for example <code>[\"kube-bench\", \"-v\", \"3\"]</code>. Once the job has run, the logs can be retrieved using <code>kubectl logs</code> on the job's pod.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>You can choose to * Run kube-bench from inside a container (sharing PID namespace with the host). See Running inside a container for additional details. * Run a container that installs kube-bench on the host, and then run kube-bench directly on the host. See Installing from a container for additional details. * install the latest binaries from the Releases page, though please note that you also need to download the config and test files from the <code>cfg</code> directory. See Download and Install binaries for details. * Compile it from source. See Installing from sources for details.</p>"},{"location":"installation/#download-and-install-binaries","title":"Download and Install binaries","text":"<p>It is possible to manually install and run kube-bench release binaries. In order to do that, you must have access to your Kubernetes cluster nodes. Note that if you're using one of the managed Kubernetes services (e.g. EKS, AKS, GKE, ACK, OCP), you will not have access to the master nodes of your cluster and you can\u2019t perform any tests on the master nodes.</p> <p>First, log into one of the nodes using SSH.</p> <p>Install kube-bench binary for your platform using the commands below. Note that there may be newer releases available. See releases page.</p> <p>Ubuntu/Debian:</p> <pre><code>curl -L https://github.com/khulnasoft-lab/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.deb -o kube-bench_0.6.2_linux_amd64.deb\n\nsudo apt install ./kube-bench_0.6.2_linux_amd64.deb -f\n</code></pre> <p>RHEL:</p> <pre><code>curl -L https://github.com/khulnasoft-lab/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.rpm -o kube-bench_0.6.2_linux_amd64.rpm\n\nsudo yum install kube-bench_0.6.2_linux_amd64.rpm -y\n</code></pre> <p>Alternatively, you can manually download and extract the kube-bench binary:</p> <pre><code>curl -L https://github.com/khulnasoft-lab/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.tar.gz -o kube-bench_0.6.2_linux_amd64.tar.gz\n\ntar -xvf kube-bench_0.6.2_linux_amd64.tar.gz\n</code></pre> <p>You can then run kube-bench directly: <pre><code>kube-bench\n</code></pre></p> <p>If you manually downloaded the kube-bench binary (using curl command above), you have to specify the location of configuration directory and file. For example: <pre><code>./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml \n</code></pre></p> <p>See previous section on Running kube-bench for further details on using the kube-bench binary.</p>"},{"location":"installation/#installing-from-sources","title":"Installing from sources","text":"<p>If Go is installed on the target machines, you can simply clone this repository and run as follows (assuming your <code>GOPATH</code> is set) as per this example:</p> <pre><code># Create a target directory for the clone, inside the $GOPATH\nmkdir -p $GOPATH/src/github.com/khulnasoft-lab/kube-bench\n\n#\u00a0Clone this repository, using SSH\ngit clone git@github.com:khulnasoft-lab/kube-bench.git $GOPATH/src/github.com/khulnasoft-lab/kube-bench\n\n#\u00a0Install the pre-requisites\ngo get github.com/khulnasoft-lab/kube-bench\n\n#\u00a0Change to the kube-bench directory\ncd $GOPATH/src/github.com/khulnasoft-lab/kube-bench\n\n#\u00a0Build the kube-bench binary\ngo build -o kube-bench .\n\n# See all supported options\n./kube-bench --help\n\n# Run all checks\n./kube-bench\n</code></pre>"},{"location":"installation/#installing-from-a-container","title":"Installing from a container","text":"<p>This command copies the kube-bench binary and configuration files to your host from the Docker container: binaries compiled for linux-x86-64 only (so they won't run on macOS or Windows) <pre><code>docker run --rm -v `pwd`:/host docker.io/khulnasoft/kube-bench:latest install\n</code></pre></p> <p>You can then run <code>./kube-bench</code>.</p>"},{"location":"platforms/","title":"Platforms","text":""},{"location":"platforms/#cis-kubernetes-benchmark-support","title":"CIS Kubernetes Benchmark support","text":"<p>kube-bench supports running tests for Kubernetes. Most of our supported benchmarks are defined in one of the following:     CIS Kubernetes Benchmarks STIG Document Library</p> <p>Some defined by other hardenening guides.</p> Source Kubernetes Benchmark kube-bench config Kubernetes versions CIS 1.5.1 cis-1.5 1.15 CIS 1.6.0 cis-1.6 1.16-1.18 CIS 1.20 cis-1.20 1.19-1.21 CIS 1.23 cis-1.23 1.22-1.23 CIS 1.24 cis-1.24 1.24 CIS 1.7 cis-1.7 1.25 CIS 1.8 cis-1.8 1.26 CIS 1.9 cis-1.9 1.27-1.29 CIS GKE 1.0.0 gke-1.0 GKE CIS GKE 1.2.0 gke-1.2.0 GKE CIS GKE 1.6.0 gke-1.6.0 GKE CIS EKS 1.0.1 eks-1.0.1 EKS CIS EKS 1.1.0 eks-1.1.0 EKS CIS EKS 1.2.0 eks-1.2.0 EKS CIS ACK 1.0.0 ack-1.0 ACK CIS AKS 1.0.0 aks-1.0 AKS RHEL RedHat OpenShift hardening guide rh-0.7 OCP 3.10-3.11 CIS OCP4 1.1.0 rh-1.0 OCP 4.1- CIS 1.6.0-k3s cis-1.6-k3s k3s v1.16-v1.24 DISA Kubernetes Ver 1, Rel 6 eks-stig-kubernetes-v1r6 EKS CIS TKGI 1.2.53 tkgi-1.2.53 vmware CIS 1.7.0-rke rke-cis-1.7 rke v1.25-v1.27 CIS 1.7.0-rke2 rke2-cis-1.6 rke2 v1.25-v1.27 CIS 1.7.0-k3s k3s-cis-1.7 k3s v1.25-v1.27"},{"location":"running/","title":"How to run","text":""},{"location":"running/#running-kube-bench","title":"Running kube-bench","text":"<p>If you run kube-bench directly from the command line you may need to be root / sudo to have access to all the config files.</p> <p>By default kube-bench attempts to auto-detect the running version of Kubernetes, and map this to the corresponding CIS Benchmark version. For example, Kubernetes version 1.15 is mapped to CIS Benchmark version <code>cis-1.15</code> which is the benchmark version valid for Kubernetes 1.15.</p> <p>kube-bench also attempts to identify the components running on the node, and uses this to determine which tests to run (for example, only running the master node tests if the node is running an API server). </p> <p>Please note It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments.</p>"},{"location":"running/#running-inside-a-container","title":"Running inside a container","text":"<p>You can avoid installing kube-bench on the host by running it inside a container using the host PID namespace and mounting the <code>/etc</code> and <code>/var</code> directories where the configuration and other files are located on the host so that kube-bench can check their existence and permissions.</p> <pre><code>docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t docker.io/khulnasoft/kube-bench:latest --version 1.18\n</code></pre> <p>Note: the tests require either the kubelet or kubectl binary in the path in order to auto-detect the Kubernetes version. You can pass <code>-v $(which kubectl):/usr/local/mount-from-host/bin/kubectl</code> to resolve this. You will also need to pass in kubeconfig credentials. For example:</p> <pre><code>docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config -t docker.io/khulnasoft/kube-bench:latest \n</code></pre> <p>You can use your own configs by mounting them over the default ones in <code>/opt/kube-bench/cfg/</code></p> <pre><code>docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t -v path/to/my-config.yaml:/opt/kube-bench/cfg/config.yaml -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config docker.io/khulnasoft/kube-bench:latest\n</code></pre>"},{"location":"running/#running-in-a-kubernetes-cluster","title":"Running in a Kubernetes cluster","text":"<p>You can run kube-bench inside a pod, but it will need access to the host's PID namespace in order to check the running processes, as well as access to some directories on the host where config files and other files are stored.</p> <p>The <code>job.yaml</code> file (available in the root directory of the repository) can be applied to run the tests as a Kubernetes <code>Job</code>. For example:</p> <pre><code>$ kubectl apply -f job.yaml\njob.batch/kube-bench created\n\n$ kubectl get pods\nNAME                      READY   STATUS              RESTARTS   AGE\nkube-bench-j76s9   0/1     ContainerCreating   0          3s\n\n# Wait for a few seconds for the job to complete\n$ kubectl get pods\nNAME                      READY   STATUS      RESTARTS   AGE\nkube-bench-j76s9   0/1     Completed   0          11s\n\n# The results are held in the pod's logs\nkubectl logs kube-bench-j76s9\n[INFO] 1 Master Node Security Configuration\n[INFO] 1.1 API Server\n...\n</code></pre> <p>To run tests on the master node, the pod needs to be scheduled on that node. This involves setting a nodeSelector and tolerations in the pod spec.</p> <p>The default labels applied to master nodes has changed since Kubernetes 1.11, so if you are using an older version you may need to modify the nodeSelector and tolerations to run the job on the master node.</p>"},{"location":"running/#running-in-an-aks-cluster","title":"Running in an AKS cluster","text":"<ol> <li> <p>Create an AKS cluster(e.g. 1.13.7) with RBAC enabled, otherwise there would be 4 failures</p> </li> <li> <p>Use the kubectl-enter plugin to shell into a node <code>kubectl-enter {node-name}</code> or ssh to one agent node could open nsg 22 port and assign a public ip for one agent node (only for testing purpose)</p> </li> <li> <p>Run CIS benchmark to view results: <pre><code>docker run --rm -v `pwd`:/host docker.io/khulnasoft/kube-bench:latest install\n./kube-bench \n</code></pre> kube-bench cannot be run on AKS master nodes</p> </li> </ol>"},{"location":"running/#running-cis-benchmark-in-an-eks-cluster","title":"Running CIS benchmark in an EKS cluster","text":"<p>There is a <code>job-eks.yaml</code> file for running the kube-bench node checks on an EKS cluster. The significant difference on EKS is that it's not possible to schedule jobs onto the master node, so master checks can't be performed</p> <ol> <li>To create an EKS Cluster refer to Getting Started with Amazon EKS in the Amazon EKS User Guide</li> <li>Information on configuring <code>eksctl</code>, <code>kubectl</code> and the AWS CLI is within</li> <li>Create an Amazon Elastic Container Registry (ECR) repository to host the kube-bench container image <pre><code>aws ecr create-repository --repository-name k8s/kube-bench --image-tag-mutability MUTABLE\n</code></pre></li> <li>Download, build and push the kube-bench container image to your ECR repo <pre><code>git clone https://github.com/khulnasoft-lab/kube-bench.git\ncd kube-bench\naws ecr get-login-password --region &lt;AWS_REGION&gt; | docker login --username AWS --password-stdin &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com\nmake build-docker IMAGE_NAME=k8s/kube-bench\ndocker tag k8s/kube-bench:latest &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest\ndocker push &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest\n</code></pre></li> <li>Copy the URI of your pushed image, the URI format is like this: <code>&lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest</code></li> <li>Replace the <code>image</code> value in <code>job-eks.yaml</code> with the URI from Step 4</li> <li>Run the kube-bench job on a Pod in your Cluster: <code>kubectl apply -f job-eks.yaml</code></li> <li>Find the Pod that was created, it should be in the <code>default</code> namespace: <code>kubectl get pods --all-namespaces</code></li> <li>Retrieve the value of this Pod and output the report, note the Pod name will vary: <code>kubectl logs kube-bench-&lt;value&gt;</code></li> <li>You can save the report for later reference: <code>kubectl logs kube-bench-&lt;value&gt; &gt; kube-bench-report.txt</code></li> </ol>"},{"location":"running/#running-disa-stig-in-an-eks-cluster","title":"Running DISA STIG in an EKS cluster","text":"<p>There is a <code>job-eks-stig.yaml</code> file for running the kube-bench node checks on an EKS cluster. The significant difference on EKS is that it's not possible to schedule jobs onto the master node, so master checks can't be performed</p> <ol> <li>To create an EKS Cluster refer to Getting Started with Amazon EKS in the Amazon EKS User Guide</li> <li>Information on configuring <code>eksctl</code>, <code>kubectl</code> and the AWS CLI is within</li> <li>Create an Amazon Elastic Container Registry (ECR) repository to host the kube-bench container image <pre><code>aws ecr create-repository --repository-name k8s/kube-bench --image-tag-mutability MUTABLE\n</code></pre></li> <li>Download, build and push the kube-bench container image to your ECR repo <pre><code>git clone https://github.com/khulnasoft-lab/kube-bench.git\ncd kube-bench\naws ecr get-login-password --region &lt;AWS_REGION&gt; | docker login --username AWS --password-stdin &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com\ndocker build -t k8s/kube-bench .\ndocker tag k8s/kube-bench:latest &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest\ndocker push &lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest\n</code></pre></li> <li>Copy the URI of your pushed image, the URI format is like this: <code>&lt;AWS_ACCT_NUMBER&gt;.dkr.ecr.&lt;AWS_REGION&gt;.amazonaws.com/k8s/kube-bench:latest</code></li> <li>Replace the <code>image</code> value in <code>job-eks-stig.yaml</code> with the URI from Step 4</li> <li>Run the kube-bench job on a Pod in your Cluster: <code>kubectl apply -f job-eks-stig.yaml</code></li> <li>Find the Pod that was created, it should be in the <code>default</code> namespace: <code>kubectl get pods --all-namespaces</code></li> <li>Retrieve the value of this Pod and output the report, note the Pod name will vary: <code>kubectl logs kube-bench-&lt;value&gt;</code></li> <li>You can save the report for later reference: <code>kubectl logs kube-bench-&lt;value&gt; &gt; kube-bench-report.txt</code></li> </ol>"},{"location":"running/#running-on-openshift","title":"Running on OpenShift","text":"OpenShift Hardening Guide kube-bench config ocp-3.10 + rh-0.7 ocp-4.1 + rh-1.0 <p>kube-bench includes a set of test files for Red Hat's OpenShift hardening guide for OCP 3.10 and 4.1. To run this you will need to specify <code>--benchmark rh-07</code>, or <code>--version ocp-3.10</code> or,<code>--version ocp-4.5</code> or <code>--benchmark rh-1.0</code> </p> <p><code>kube-bench</code> supports auto-detection, when you run the <code>kube-bench</code> command it will autodetect if running in openshift environment.</p> <p>Since running <code>kube-bench</code> requires elevated privileges, the <code>privileged</code> SecurityContextConstraint needs to be applied to the ServiceAccount used for the <code>Job</code>:</p> <pre><code>oc create namespace kube-bench\noc adm policy add-scc-to-user privileged --serviceaccount default\noc apply -f job.yaml\n</code></pre>"},{"location":"running/#running-in-a-gke-cluster","title":"Running in a GKE cluster","text":"CIS Benchmark Targets gke-1.0 master, controlplane, node, etcd, policies, managedservices gke-1.2.0 master, controlplane, node, policies, managedservices gke-1.6.0 master, controlplane, node, policies, managedservices <p>kube-bench includes benchmarks for GKE. To run this you will need to specify <code>--benchmark gke-1.0</code>, <code>--benchmark gke-1.2.0</code> or <code>--benchmark gke-1.6.0</code> when you run the <code>kube-bench</code> command.</p> <p>To run the benchmark as a job in your GKE cluster apply the included <code>job-gke.yaml</code>.</p> <pre><code>kubectl apply -f job-gke.yaml\n</code></pre>"},{"location":"running/#running-in-a-ack-cluster","title":"Running in a ACK cluster","text":"CIS Benchmark Targets ack-1.0 master, controlplane, node, etcd, policies, managedservices <p>kube-bench includes benchmarks for Alibaba Cloud Container Service For Kubernetes (ACK). To run this you will need to specify <code>--benchmark ack-1.0</code> when you run the <code>kube-bench</code> command.</p> <p>To run the benchmark as a job in your ACK cluster apply the included <code>job-ack.yaml</code>.</p> <pre><code>kubectl apply -f job-ack.yaml\n</code></pre>"},{"location":"running/#running-in-a-vmware-tkgi-cluster","title":"Running in a VMware TKGI cluster","text":"CIS Benchmark Targets tkgi-1.2.53 master, etcd, controlplane, node, policies <p>kube-bench includes benchmarks for VMware tkgi platform. To run this you will need to specify <code>--benchmark tkgi-1.2.53</code> when you run the <code>kube-bench</code> command.</p> <p>To run the benchmark as a job in your VMware tkgi cluster apply the included <code>job-tkgi.yaml</code>.</p> <pre><code>kubectl apply -f job-tkgi.yaml\n</code></pre>"},{"location":"running/#running-in-a-rancher-rke-cluster","title":"Running in a Rancher RKE cluster","text":"CIS Benchmark Targets rke-cis-1.7 master, etcd, controlplane, node, policies <p>kube-bench includes benchmarks for Rancher RKE platform. To run this you will need to specify <code>--benchmark rke-cis-1.7</code> when you run the <code>kube-bench</code> command.</p>"},{"location":"running/#running-in-a-rancher-rke2-cluster","title":"Running in a Rancher RKE2 cluster","text":"CIS Benchmark Targets rke2-cis-1.7 master, etcd, controlplane, node, policies <p>kube-bench includes benchmarks for Rancher RKE2 platform. To run this you will need to specify <code>--benchmark rke2-cis-1.7</code> when you run the <code>kube-bench</code> command.</p>"},{"location":"running/#running-in-a-rancher-k3s-cluster","title":"Running in a Rancher K3s cluster","text":"CIS Benchmark Targets k3s-cis-1.7 master, etcd, controlplane, node, policies <p>kube-bench includes benchmarks for Rancher K3S platform. To run this you will need to specify <code>--benchmark k3s-cis-1.7</code> when you run the <code>kube-bench</code> command.</p>"}]}